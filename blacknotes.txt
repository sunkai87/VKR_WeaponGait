=== Проект: Распознавание скрытого ношения оружия по походке ===

Общая цель:
Создать интеллектуальную систему, способную по видеопотоку выявлять людей, скрыто несущих оружие, на основе анализа их походки.



Сбор и разметка датасета: YouTube и Unity


Классы: no_weapon — нормальная походка; weapon — скрытое оружие.


Ключевые особенности проекта:

Работа преимущественно на CPU

Целевая задача — не просто распознавание походки, а выявление скрытого оружия.

Основной подход — скелетные методы (pose-based).

Ориентация на практическую интеграцию в системы видеонаблюдения.


----

Как обновлять манифест:

Get-ChildItem videos\*.mp4 | ForEach-Object {
    $relativePath = "videos\" + $_.Name

    if ($_.Name -match '^n') {
        $label = 'no_weapon'
    } else {
        $label = 'weapon'
    }

    "$relativePath,$label"
} | Out-File data\manifest.csv -Encoding utf8


----

Как обрабатывать с mediapipe. Проверить результат можно в cache/poses_mp/

# PowerShell -- Это сразу все и заново
Get-ChildItem videos\*.mp4 | % {
    python -m weapon_gait.cli extract --video $_ --backend mediapipe
}


Это обработка только новых роликов (здесь приведено для w_...)

Get-ChildItem videos\\w_*.mp4 | Where-Object {
    -not (Test-Path ("cache\\poses_mp\\" + $_.BaseName + ".npy"))
} | ForEach-Object {
    python -m weapon_gait.cli extract --video $_ --backend mediapipe
}

----

# (A) извлекаем позы одного файла
python -m weapon_gait.cli extract --video videos\n_001.mp4 --backend mediapipe


# Удаление старого кэша у RF:
Remove-Item cache\\features_mediapipe_stats\\manifest.csv -ErrorAction Ignore

# (B) обучаем RandomForest
python -m weapon_gait.cli train --manifest data\manifest.csv `
                                --pose-backend mediapipe `
                                --gait-backend stats `
                                --model runs\rf.joblib

# (C) предсказываемs
python -m weapon_gait.cli predict --video videos\n_001.mp4 --model runs\rf.joblib


Get-ChildItem videos\\w_*.mp4 | % {
    $stem = $_.BaseName
    $npy  = \"cache\\poses_movenet\\$stem.npy\"
    if (-not (Test-Path $npy)) {
        python -m weapon_gait.cli extract --video $_ --backend movenet
    }
}


Get-ChildItem videos\\w_*.mp4 | Where-Object {
    -not (Test-Path ("cache\\poses_movenet\\" + $_.BaseName + ".npy"))
} | ForEach-Object {
    python -m weapon_gait.cli extract --video $_ --backend movenet
}



----
Обучение MoveNet
C:\Users\vkash\Desktop\JupyterWorks\VKR> python -m weapon_gait.cli train `
>>        --manifest data\manifest.csv `
>>        --pose-backend movenet `
>>        --gait-backend stats17 `
>>        --model runs\rf_movenet17.joblib

Предсказание МувНет

python -m weapon_gait.cli predict --video videos\w_020.mp4 --model runs\rf_movenet17.joblib


---
YoloPose

# 4-a. извлекаем
python -m weapon_gait.cli extract --video videos/w_022.mp4 --backend yolopose

# 4-b. тренируем
python -m weapon_gait.cli train \
       --manifest data/manifest.csv \
       --pose-backend yolopose \
       --gait-backend stats17 \        # 17 точек
       --model runs/rf_yolo17.joblib


---
openpifpaf

1 ролик
python -m weapon_gait.cli extract --video videos/w_022.mp4 --backend pifpaf  


Обучение
python -m weapon_gait.cli train \
       --manifest data/manifest.csv \
       --pose-backend pifpaf \
       --gait-backend stats17 \
       --model runs/rf_pifpaf17.joblib



Как запускать cli.py

#SVM на MediaPipe-Stats (+аугментация ×3) ["rf", "svm", "nb", "logreg", "gb"]
python -m weapon_gait.cli train-ml `
       --manifest data\manifest.csv `
       --pose-backend mediapipe `
       --gait-backend stats `
       --model-key svm `
       --augment-factor 3

Надо также менять название runs rf_stats \ svm_stats \ nb_stats \ logreg_stats \ gb_stats


Что делать, когда закинул ролики

Короткая памятка на будущее
+ видео → 2. manifest → 3. extract poses

удалить features CSV → 5. train-ml



Как можно запускать Трэйн

python -m weapon_gait.cli train-ml `
       --manifest data\manifest.csv `
       --pose-backend mediapipe `
       --gait-backend stats `
       --model-key rf `
       --augment-factor 3 `
       --grid-search `
       --rebuild-features

---
Обучение с частичным перекрытием

python -m weapon_gait.cli train-ml `
       --manifest data\manifest.csv `
       --pose-backend mediapipe `
       --gait-backend statsparts `
       --model-key gb `
       --augment-factor 3 `
       --grid-search `
       --rebuild-features


python -m weapon_gait.cli train-ml `
       --manifest data\manifest.csv `
       --pose-backend mediapipe `
       --gait-backend statsplus `
       --model-key gb `
       --augment-factor 3 `
       --grid-search `
       --kfold 5 `
       --rebuild-features `
       --plots


Пример хорошего запуска:

python -m weapon_gait.cli train-ml `
       --manifest data\manifest.csv `
       --pose-backend mediapipe `
       --gait-backend statsplus `
       --model-key gb `
       --augment-factor 0 `
       --kfold 4 `
       --grid-search `
       --rebuild-features `
       --plots

-------------------------------------------------

Теперь работаем с кропами

python -m weapon_gait.cli extract `
       --video videos/n_001.mp4 `
       --backend crop_mp


---
Это обработка только новых роликов (здесь приведено для w_...)

Get-ChildItem videos\n_*.mp4 | Where-Object {
    $stem = $_.BaseName
    $matches = Get-ChildItem "cache\poses_crop_mp" -Filter "$stem`_pid*.npy"
    $matches.Count -eq 0
} | ForEach-Object {
    python -m weapon_gait.cli extract --video $_ --backend crop_mp
}


Или так:


---
Обучение

.npy файлы → pose_backend npy

.mp4 ролики → pose_backend crop_mp (или другой одиночный backend)

Для кроп людей
python -m weapon_gait.cli train-ml `
       --manifest data\manifest_crop.csv `
       --pose-backend npy `
       --gait-backend statsplus `
       --model-key gb `
       --augment-factor 0 `
       --grid-search `
       --kfold 4 `
       --rebuild-features `
       --plots

Старая версия для 1 человека
python -m weapon_gait.cli train-ml `
       --manifest data\manifest.csv `
       --pose-backend mediapipe `
       --gait-backend statsplus `
       --model-key gb `
       --augment-factor 0 `
       --grid-search `
       --kfold 4 `
       --rebuild-features `
       --plots


---
Предсказание с Визуализацией

python -m weapon_gait.cli predict `
       --video videos/n_130.mp4 `
       --model runs/gb_statsplus.joblib `
       --out-vis visualize/vis_n_130.mp4



Теперь работаем с RNN

python -m weapon_gait.rnn_cli train `
       --arch lstm|gru|tcn|bilstm `
       --manifest data\manifest_crop_npy.csv `
       --feat-backend inst_stats `
       --epochs 20 --batch 64


python -m weapon_gait.rnn_cli predict `
       --video videos\n_130.mp4 `
       --model runs\gru_inst_stats.pt


python -m weapon_gait.rnn_cli train `
       --arch lstm `
       --manifest data\manifest_crop.csv `
       --feat-backend inst_stats `
       --epochs 20 --batch 64




python GUI\server\classic_runner.py `
       videos\n_001.mp4 `
       vis.mp4 `
       out.json `
       rf `
       "--plots"


---
Сделал новую ветку в моменте до lstm




Давай сейчас займемся тем, что будем оптимизировать процесс выделения нескольких людей одновременно. 
Дело в том, я попробовал предсказать видеоролик c классом weapon с помощью старого способа с бэкэндом MediaPipe. Результат предсказания был 86%, что это weapon класс (таким он и есть). 
Затем я попробовал сделать предсказание с помощью нашего новой бэкенда npy. И  она предсказала этот ролик как класс no_weapon (weapon был с вероятностью 34%). В итоге очень сильно снизилось качество системы, хотя я выбирал одинаковые параметры и схожий размер датасета. 
Мне кажется, что мы на каком-то этапе выделения человека, мы делаем что-то не так и, возможно, обрезаем какие-то его части. Надо разобраться.



Давай для начала составим план, чтобы в итоге улучшить это выделение людей, вот это обработку сразу нескольких людей.

Какие еще слабые места нам надо проверить? Может быть какие-то проверки поставить? 

Я тебе получается сам предложил свою версию, однако она может быть неверной. Что ты сам думаешь по этому поводу? 



---
Да, в целом хорошо

Знаешь, хочу отметить, что это неполный перечень того, что я бы хотел описать. Просто можешь опираться на эту структуру и дополнять теми разделами, которые считаешь нужными

---
Знаешь, мне кажется, нужно каждый этап отдельно рассматривать (Потому что они много в себе важных деталей заключают)



Пользователь пишет диплом по проекту системы распознавания наличия оружия у человека по походке. Структура диплома разделена на две главы: 
Глава 1 — проектирование и разработка системы (входные данные, сбор и обработка датасета, описание этапов, классические методы и сравнение, инструменты реализации); Глава 2 — оптимизация системы 
(аугментация, подбор параметров, графики, многолюдные сцены, GUI, метрики). Рекуррентные сети пока не рассматриваются. В заключении будут подведены итоги.